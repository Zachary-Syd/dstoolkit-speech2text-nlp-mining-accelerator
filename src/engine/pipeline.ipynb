{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# BHP Positive Comms Proof-of-Concept sample pipeline notebook \n",
        "\n",
        "This notebook outlines the process to build an AMLS pipeline to undertake the transcibing of the battle scenario.\n",
        "\n",
        "The actions to be performed are:\n",
        " - Importing Python packages required to build a pipeline\n",
        " - Creating/Connecting to an AMLS workspace\n",
        " - Creating the experiment for the pipeline to use\n",
        " - Building the pipeline\n",
        " - Deploy the pipeline\n",
        " - Monitor the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nej-compute-private4/code/Users/nejhdeh.ghevondian/bhp-positive-comms-poc/positive-comms-experimentation/src\n",
            "Azure ML version: 1.37.0\n",
            "Loading environmental variables True\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# setup the current paths\n",
        "import os, sys\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "currentDir = os.path.dirname(os.getcwd())\n",
        "print(f'Current working directory: {currentDir}')\n",
        "sys.path.append('../')\n",
        "sys.path.append('./')\n",
        "\n",
        "# azure ML core services\n",
        "import azureml.core\n",
        "from azureml.core import Experiment, Workspace\n",
        "\n",
        "# Azure ML pipline \n",
        "from azureml.pipeline.core import Pipeline, PipelineParameter\n",
        "\n",
        "# import from common setups & environments\n",
        "from common.constants import *\n",
        "from common.azureml_configuration import *\n",
        "from common.general_utilities import *\n",
        "\n",
        "# load the env variables from the hidden file\n",
        "print('Loading environmental variables', load_dotenv(find_dotenv('./common/.env')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating/Connecting to the AMLS workspace\n",
        "\n",
        "Next step is to create (or connect to existing) the AMLS workspace where the experiment pipeline will be deployed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. configure the azure ml workspace\n",
        "#------------------------------------\n",
        "# get subscription id and other keys from .env file, Other constabst are from source files\n",
        "TENANT_ID = os.environ.get('TENANT_ID')\n",
        "SUBSCRIPTION_ID = os.environ.get('SUBSCRIPTION_ID')\n",
        "\n",
        "DOCKER_FILE = 'docker_dependencies.yml'\n",
        "DOCKER_FILE_PATH = f'../{PIPELINE_SCRIPT_PATH}/{DOCKER_FILE}'\n",
        "\n",
        "# create a temp directory to store results with sub-foldrs\n",
        "#---------------------------------------------------------\n",
        "utilConfig = GeneraltUtilities()\n",
        "\n",
        "print('Configuring the Azure ML services and workplace, pleat wait...')\n",
        "\n",
        "dsp_inference_folders = f'{INFERENCE_PATH}{RESULTS_RECORDINGS_INFERENCE_DSP_PATH}'\n",
        "transcripts_inference_folder = f'{INFERENCE_PATH}{RESULTS_RECORDINGS_INFERENCE_TRANSCRIBE_PATH}'\n",
        "assessed_inference_folder =  f'{INFERENCE_PATH}{RESULTS_RECORDINGS_INFERENCE_ASSESSED_PATH}'\n",
        "\n",
        "utilConfig.createTmpDir(dsp_inference_folders)\n",
        "utilConfig.createTmpDir(transcripts_inference_folder)\n",
        "utilConfig.createTmpDir(assessed_inference_folder)\n",
        "\n",
        "# initilaise the azureml config class\n",
        "azuremlConfig = AzureMLConfiguration(workspace=WORKSPACE_NAME, tenant_id=TENANT_ID, subscription_id=SUBSCRIPTION_ID, \n",
        "                                    resource_group=RESOURCE_GROUP, location=LOCATION)\n",
        "# configure Azure ML services\n",
        "#----------------------------\n",
        "# configure Azure ML workspace\n",
        "azuremlConfig.configWorkspace()\n",
        "\n",
        "# configure the azure ML compute \n",
        "azuremlConfig.configCompute()\n",
        "\n",
        "# configure the experiment(s)\n",
        "azuremlConfig.configExperiment(experiment_name=EXPERIMENT_NAME)\n",
        "\n",
        "# configure the environment - conda\n",
        "azuremlConfig.configEnvironment(environment_name=ENVIRONMENT_NAME)\n",
        "\n",
        "# confogure and register the datastore(s) with Azure ML piplines\n",
        "inference_datastore = azuremlConfig.configDataStore(datastore=INFERENCE_DATASTORE_NAME, container_name=INFERENCE_CONTAINER_NAME)\n",
        "processed_datastore = azuremlConfig.configDataStore(datastore=DSP_INFERENCE_DATASTORE_NAME, container_name=DSP_INFERENCE_CONATINER_NAME)\n",
        "transcribed_datastore = azuremlConfig.configDataStore(datastore=TRANSCRIBED_INFERENCE_DATASTORE_NAME, container_name=TRANSCRIBED_INFERENCE_CONATINER_NAME)\n",
        "assessed_datastore = azuremlConfig.configDataStore(datastore=ASSESSED_INFERENCE_DATASTORE_NAME, container_name=ASSESSED_INFERENCE_CONATINER_NAME)\n",
        "\n",
        "#register the datasets associated with the datastore - recordings\n",
        "inference_recordings_datasets = azuremlConfig.configDatasets(datastore=inference_datastore, file_path= INFERENCE_RECORDINGS_FOLDER, \n",
        "                                            dataset_name=INFERENCE_RECORDINGS_DATASET_NAME, description='inference recordings datasets')\n",
        "\n",
        "# register the datasets associated with the datastore - key phrases\n",
        "key_phrases_datasets = azuremlConfig.configDatasets(datastore=inference_datastore, file_path = KEY_PHRASES_FOLDER, \n",
        "                                            dataset_name=INFERENCE_KEY_PHRASES_DATASET_NAME, description='inference key phrases datasets')\n",
        "\n",
        "# register the datasets associated with the datastore - key phrases\n",
        "assessed_datasets = azuremlConfig.configDatasets(datastore=assessed_datastore, file_path = ASSESSED_FOLDER, \n",
        "                                            dataset_name=INFERENCE_ASSESSED_DATASET_NAME, description='inference assessed datasets')\n",
        "\n",
        "# configre the datasets to be used for the pieline data\n",
        "processed_ds = azuremlConfig.configPipelineOutputData(name='processed_data', destination=(processed_datastore, '/'))\n",
        "transcribed_ds = azuremlConfig.configPipelineOutputData(name='transcribed_data', destination=(transcribed_datastore, '/'))\n",
        "assessed_ds = azuremlConfig.configPipelineOutputData(name='nlp_data', destination=(assessed_datastore, '/'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure th Azure ML environment to run the expirement in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup the pipeline configuration parameters\n",
        "azuremlConfig.configPipeline(docker_path=DOCKER_FILE_PATH, conda_packages=CONDA_PACKAGES, pip_packages=PIP_PACKAGES)\n",
        "\n",
        "# configure the pipeline data paths\n",
        "datapath_pipeline_param, datapath_input = azuremlConfig.configPipelineInputData(datastore=inference_datastore, path_on_datastore=INFERENCE_RECORDINGS_FOLDER)\n",
        "\n",
        "# setup the pipeine parameters\n",
        "# Parameter 1\n",
        "pipeline_lang_param = azuremlConfig.configPipelineParameter(description='language used to translate the Speech to Text. (e.g. en-AU, en-GB, en-US)', \n",
        "                                                            default_value='en-US')\n",
        "# Parameter 2\n",
        "pipeline_validate_param = azuremlConfig.configPipelineParameter(description='If truth validation file exists, set to True.', \n",
        "                                                            default_value=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup the correct path to align the pipline entry\n",
        "current_dir = os.getcwd()\n",
        "os.chdir('../..')\n",
        "\n",
        "# setup the script steps\n",
        "filter_step = PythonScriptStep(name='Step 1 - Filter Audio Files',\n",
        "                               script_name = FILTER_SCRIPT_FILENAME,\n",
        "                               arguments = [ '--raw_input_datapath', datapath_input, '--processed_dir', processed_ds], \n",
        "                               inputs = [datapath_input],\n",
        "                               outputs = [processed_ds],  \n",
        "                               source_directory = PIPELINE_SOURCE_PATH,\n",
        "                               compute_target = azuremlConfig.compute_target,\n",
        "                               runconfig = azuremlConfig.aml_run_config,\n",
        "                               allow_reuse = True)\n",
        " \n",
        "transcribe_step = PythonScriptStep(name='Step 2 - Transcribe Filtered Audio Files',\n",
        "                               script_name = TRANSCRIBE_SCRIPT_FILENAME,\n",
        "                               arguments = ['--validate', pipeline_validate_param,'--language', pipeline_lang_param, '--processed_data', processed_ds.as_input('filtered_audio_files_ds').as_mount(), \n",
        "                                        '--key_phrases_path', key_phrases_datasets.as_mount(), '--transcribed_data', transcribed_ds], \n",
        "                               outputs = [transcribed_ds],  \n",
        "                               source_directory = PIPELINE_SOURCE_PATH,\n",
        "                               compute_target = azuremlConfig.compute_target,\n",
        "                               runconfig = azuremlConfig.aml_run_config,\n",
        "                               allow_reuse = True)\n",
        "\n",
        "nlp_step = PythonScriptStep(name='Step 3 - NLP Anaysis',\n",
        "                               script_name = NLP_SCRIPT_FILENAME,\n",
        "                               arguments = ['--transcribed_data', transcribed_ds.as_input('transcribed_audio_files_ds').as_mount(), '--nlp_data', assessed_ds],  \n",
        "                               outputs = [assessed_ds],  \n",
        "                               source_directory = PIPELINE_SOURCE_PATH,\n",
        "                               compute_target = azuremlConfig.compute_target,\n",
        "                               runconfig = azuremlConfig.aml_run_config,\n",
        "                               allow_reuse = True)\n",
        "\n",
        "ML_step = PythonScriptStep(name='Step 4 - Comms Classification',\n",
        "                               script_name = ML_SCRIPT_FILENAME,\n",
        "                               arguments = ['--nlp_data', assessed_ds.as_input('nlp_ds').as_mount()],  \n",
        "                               source_directory = PIPELINE_SOURCE_PATH,\n",
        "                               compute_target = azuremlConfig.compute_target,\n",
        "                               runconfig = azuremlConfig.aml_run_config,\n",
        "                               allow_reuse = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build the pipeline and run the experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created step Step 1 - Filter Audio Files [bffee21f][db58e25a-186d-4431-b799-9600327f8dfa], (This step will run and generate new outputs)\n",
            "Created step Step 2 - Transcribe Filtered Audio Files [d5befb72][5cd3fe3e-cae0-4721-be38-e64acb6b85c1], (This step will run and generate new outputs)Created step Step 3 - NLP Anaysis [d0a9fcdb][d1109603-dcd3-4fc1-af51-61421704f1bc], (This step will run and generate new outputs)\n",
            "\n",
            "Created step Step 4 - Comms Classification [dea6e69c][653feb66-1cbc-4ba1-aa4c-fb05d2630ab6], (This step will run and generate new outputs)\n",
            "Created data reference inference_datastore_16635bfa for StepId [86525019][889f1187-009b-475e-993e-138004c32738], (Consumers of this data will generate new runs.)\n",
            "Submitted PipelineRun a5eb09cc-e37f-483d-a514-364103a88a81\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/a5eb09cc-e37f-483d-a514-364103a88a81?wsid=/subscriptions/26256615-d27f-4067-92b9-53a9d6126c57/resourcegroups/it-aue1-pov-arg-poscomms/workspaces/pos-comms-poc-aml&tid=4f6e1565-c2c7-43cb-8a4c-0981d022ce20\n"
          ]
        }
      ],
      "source": [
        "# list of steps to run\n",
        "pipeline_steps = [filter_step, transcribe_step, nlp_step, ML_step]\n",
        "\n",
        "# build the pipeline\n",
        "pipeline = azuremlConfig.buildPipeline(azuremlConfig.ws, script_steps = pipeline_steps)\n",
        "\n",
        "# run the pipeline\n",
        "#run =  azuremlConfig.submitPipeline(azuremlConfig.experiment, pipeline, regenerate_outputs=True)\n",
        "\n",
        "run = azuremlConfig.experiment.submit(pipeline, regenerate_outputs=True)\n",
        "\n",
        "# revert to current directory\n",
        "os.chdir(current_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Publish Pipeline\n",
        "Publishing a pipeline allows the pipeline to be run by other uses with different inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "published_pipeline = pipeline.publish(\n",
        "     name='Positive Comms Classification',\n",
        "     description=\"Takes raw audio files as input. Then filters, transcribes, applies NLP (Natural Language Processing) and comms analysis to create a dataframe output.\",\n",
        "     version=\"1.0\",\n",
        "     continue_on_step_failure=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create a versioned pipeline endpoint\n",
        "\n",
        "You can create a Pipeline Endpoint with multiple published pipelines behind it. This technique gives you a fixed REST endpoint as you iterate on and update your ML pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import PipelineEndpoint\n",
        "from azureml.pipeline.core import PublishedPipeline\n",
        "\n",
        "published = pipeline.publish(name='Positive Comms Classification')\n",
        "pipeline_endpoint = PipelineEndpoint.get(workspace=azuremlConfig.ws, name='Positive Comms Classification')\n",
        "pipeline_endpoint.add_default(published)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trigger a pipeline\n",
        "Time-based schedules can be used to take care of routine tasks, such as monitoring for data drift. Change-based schedules can be used to react to irregular or unpredictable changes, such as new data being uploaded or old data being edited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "from azureml.pipeline.core import Pipeline, PublishedPipeline\n",
        "from azureml.core.experiment import Experiment\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "experiments = Experiment.list(ws)\n",
        "for experiment in experiments:\n",
        "    print(experiment.name)\n",
        "\n",
        "published_pipelines = PublishedPipeline.list(ws)\n",
        "for published_pipeline in  published_pipelines:\n",
        "    print(f\"{published_pipeline.name},'{published_pipeline.id}'\")\n",
        "\n",
        "experiment_name = \"testpipeline1\" \n",
        "pipeline_id = \"74d1d177-2d58-4ab2-9331-4b6b1885223d\" \n",
        "\n",
        "# Create a schedule\n",
        "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
        "# Change-Based Schedule\n",
        "datastore = Datastore(workspace=ws, name=\"dev_raw_datastore\")\n",
        "reactive_schedule = Schedule.create(ws, name=\"MyReactiveSchedule\", description=\"Based on input file change.\",\n",
        "                            pipeline_id=pipeline_id, experiment_name=experiment_name, datastore=datastore, path_on_datastore='/radio-check') #data_path_parameter_name=\"input_data\")\n",
        "\n",
        "# To disable the schedule\n",
        "reactive_schedule.update(status='Disabled')\n",
        "\n",
        "# To list all schedules for a pipeline\n",
        "f = Schedule.list(ws,pipeline_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir = '/home/azureuser/cloudfiles/code/Users/nejhdeh.ghevondian/bhp-positive-comms-poc/positive-comms-experimentation/src/engine'\n",
        "os.chdir(dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Submit a job to a pipeline endpoint\n",
        "\n",
        "You can submit a job to the default version of a pipeline endpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Default version\n",
        "pipeline_endpoint_by_name = PipelineEndpoint.get(workspace=ws, name=\"PipelineEndpointTest\")\n",
        "run_id = pipeline_endpoint_by_name.submit(\"testpipeline1\")\n",
        "print(run_id)\n",
        "\n",
        "# or to a specific version\n",
        "#run_id = pipeline_endpoint_by_name.submit(\"PipelineEndpointExperiment\", pipeline_version=\"0\")\n",
        "#print(run_id)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
